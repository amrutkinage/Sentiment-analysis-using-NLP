{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Objective: To predict whether a a tweet sentiment is a negavtive or positive\n",
    "Used Python 3 and Anaconda Jupyter Notebook For Creating The Poject\n",
    "The Steps followed by me throughout the project were as follows\n",
    "1) Understanding the Dataset\n",
    "2) Importing The required Models\n",
    "3) Data Cleaning (treating missing values, check for colrelations,dropping less sisgnificant values)\n",
    "4) Model Building\n",
    "5) Prediction\n",
    "6) Checking the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('sentiment.tsv', sep='\\t',names=['senti', 'tweet'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@jamielewislewis i cant believe it, it really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>having a vodka tonic and looking forward to go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>@ddlovatofans1neg1 Could you follow me please....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>@jordanknight for once.................. PLEAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>Had a dream about a walk in fast food resturau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  senti                                              tweet\n",
       "0   neg  @jamielewislewis i cant believe it, it really ...\n",
       "1   pos  having a vodka tonic and looking forward to go...\n",
       "2   pos  @ddlovatofans1neg1 Could you follow me please....\n",
       "3   pos  @jordanknight for once.................. PLEAS...\n",
       "4   neg  Had a dream about a walk in fast food resturau..."
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     senti                                              tweet\n",
      "0      neg  @jamielewislewis i cant believe it, it really ...\n",
      "1      pos  having a vodka tonic and looking forward to go...\n",
      "2      pos  @ddlovatofans1neg1 Could you follow me please....\n",
      "3      pos  @jordanknight for once.................. PLEAS...\n",
      "4      neg  Had a dream about a walk in fast food resturau...\n",
      "5      pos  @TroyBrownBBNews Yes... For a &quot;friend&quot; \n",
      "6      pos                       Packing for work expierance \n",
      "7      neg  hates @internet @explrer (angry)(angry) **but ...\n",
      "8      neg  @federalcase  I said I go out for eat 5:negneg...\n",
      "9      neg  @babykates7 yeah they won't do the surgery til...\n",
      "10     pos       @BritRuxpin. I say answering with no pants. \n",
      "11     neg  Am in the place called negUT OF NOW WHERE!! Ta...\n",
      "12     neg  3am, still up.. flying to LA ltr 8am..hayyy sl...\n",
      "13     neg  2 down, 8 to go...looks like i wont be getting...\n",
      "14     neg     It's Increible How Someone Forgot U Soo Fast! \n",
      "15     neg  That little girl holly!  find some time! Aw, s...\n",
      "16     neg  Aapko huyi asuvidha ke liye hume khed hay Trai...\n",
      "17     neg  @brightondoll ugh, me too. 3:pos5 and going st...\n",
      "18     neg          Will be back when I think everyone is up \n",
      "19     pos  &quot;so we keep waitin, waitin , waitin on th...\n",
      "20     neg  @carposdave sinus infection.    the whole left...\n",
      "21     neg  @twilighterNY I've seen the pics but only on m...\n",
      "22     neg  @Streyeder I wish I had time or extracurricula...\n",
      "23     pos  @ms_cornwall night you, may your dreams be won...\n",
      "24     pos  Star Wars: Old Republic: http://digg.com/d1sgu...\n",
      "25     pos                You could be my punk rock princess \n",
      "26     neg  @YukiB Iknow, I am just jealous and hate to be...\n",
      "27     pos  19 more sleeps untill Take That ahhhhhhhhhhhhhhh \n",
      "28     pos  aaaw! my moms crying. She loved the mixed tape...\n",
      "29     neg                 @Velf79 Missed you last night hon \n",
      "...    ...                                                ...\n",
      "1971   pos            @Animadi You're welcome and thank you. \n",
      "1972   neg  Another rainy day in Lititz, PA.    On the up ...\n",
      "1973   neg  @dovwaterman I'm not a BAFTA member so clearly...\n",
      "1974   pos                        @zealandsmom Thx for that! \n",
      "1975   neg  On my way to school... Thinking I may have don...\n",
      "1976   pos  @toyhorses Exellent, I'll be over about 1:3neg...\n",
      "1977   pos  watching im a celebrity.....get me out of here...\n",
      "1978   pos  @davynathan of course we r!! BHs r nothing if ...\n",
      "1979   pos     @MrPeterAndre no problem Pete hope you are ok \n",
      "1980   pos  @successfool You know it PIC!  I believe in yo...\n",
      "1981   pos                               Hanging out at home \n",
      "1982   neg  Debating taking the dogs down to bed &amp; giv...\n",
      "1983   pos  @Andreaheartscgh @Catherine_andy Remember to y...\n",
      "1984   neg              Caps  Well, there's always next year.\n",
      "1985   pos  @khallwalker experience shows that sending the...\n",
      "1986   pos                             bye bye twitter world \n",
      "1987   neg  @ScruffyPanther oh no !  blocking them didn't ...\n",
      "1988   neg             Not feelin great at alll this morning \n",
      "1989   pos  just got done editing pix from yesterday's #gb...\n",
      "1990   neg  All these good/new shows on tonight....and I'm...\n",
      "1991   pos  @tinhet Hey you      It surprisingly didn't hu...\n",
      "1992   pos  @childrensjewell @Bazzaa  Thank you  I will po...\n",
      "1993   pos    Just smashed this Tommy's for my 2PM breakfast \n",
      "1994   pos  @ionacosmetics spent yesterday outside myself-...\n",
      "1995   neg  @PrimalMan You're so lucky! I used to have acc...\n",
      "1996   neg  STILL @ panera...studying for &quot;mock&quot;...\n",
      "1997   neg  Insomnia is out of control tonight--haven't sl...\n",
      "1998   pos          @Covergirlneg8 I take pride in what I do \n",
      "1999   pos                          heading to work on the 6 \n",
      "2000   pos                                   @queith asi es! \n",
      "\n",
      "[2001 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    @jamielewislewis i cant believe it, it really ...\n",
       "1    having a vodka tonic and looking forward to go...\n",
       "2    @ddlovatofans1neg1 Could you follow me please....\n",
       "3    @jordanknight for once.................. PLEAS...\n",
       "4    Had a dream about a walk in fast food resturau...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "data['tweet'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Removal of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @jamielewislewis cant believe it, really doesn...\n",
       "1    vodka tonic looking forward going Saddle Ranch...\n",
       "2    @ddlovatofans1neg1 Could follow please.I would...\n",
       "3    @jordanknight once.................. PLEASE TE...\n",
       "4    Had dream walk fast food resturaunt sold ice c...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  calculating the number of hashtags or mentions present in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hastags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@jamielewislewis cant believe it, really doesn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vodka tonic looking forward going Saddle Ranch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ddlovatofans1neg1 Could follow please.I would...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@jordanknight once.................. PLEASE TE...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Had dream walk fast food resturaunt sold ice c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  hastags\n",
       "0  @jamielewislewis cant believe it, really doesn...        0\n",
       "1  vodka tonic looking forward going Saddle Ranch...        0\n",
       "2  @ddlovatofans1neg1 Could follow please.I would...        0\n",
       "3  @jordanknight once.................. PLEASE TE...        0\n",
       "4  Had dream walk fast food resturaunt sold ice c...        0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hastags'] = data['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "data[['tweet','hastags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### pre-processing step which we will do is transform our tweets into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @jamielewislewis cant believe it, really doesn...\n",
       "1    vodka tonic looking forward going saddle ranch...\n",
       "2    @ddlovatofans1neg1 could follow please.i would...\n",
       "3    @jordanknight once.................. please te...\n",
       "4    had dream walk fast food resturaunt sold ice c...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jamielewislewis cant believe it really doesnt ...\n",
       "1    vodka tonic looking forward going saddle ranch...\n",
       "2    ddlovatofans1neg1 could follow pleasei would r...\n",
       "3    jordanknight once please tell us why u thinkin...\n",
       "4    had dream walk fast food resturaunt sold ice c...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].str.replace('[^\\w\\s]','')\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### calculate the number of numerics which are present in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jamielewislewis cant believe it really doesnt ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vodka tonic looking forward going saddle ranch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ddlovatofans1neg1 could follow pleasei would r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jordanknight once please tell us why u thinkin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>had dream walk fast food resturaunt sold ice c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  numerics\n",
       "0  jamielewislewis cant believe it really doesnt ...         0\n",
       "1  vodka tonic looking forward going saddle ranch...         0\n",
       "2  ddlovatofans1neg1 could follow pleasei would r...         0\n",
       "3  jordanknight once please tell us why u thinkin...         0\n",
       "4  had dream walk fast food resturaunt sold ice c...         0"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['numerics'] = data['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "data[['tweet','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  removing commonly occurring words from our text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i        611\n",
       "im       234\n",
       "good     102\n",
       "today     99\n",
       "get       98\n",
       "like      95\n",
       "work      85\n",
       "love      83\n",
       "back      81\n",
       "got       81\n",
       "dtype: int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(data['tweet']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jamielewislewis cant believe it really doesnt ...\n",
       "1    vodka tonic looking forward going saddle ranch...\n",
       "2    ddlovatofans1neg1 could follow pleasei would r...\n",
       "3    jordanknight once please tell us why u thinkin...\n",
       "4    had dream walk fast food resturaunt sold ice c...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  removing rarely occurring words from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classroom              1\n",
       "culvers                1\n",
       "keenthanx              1\n",
       "oohh                   1\n",
       "pshhhht                1\n",
       "betcha                 1\n",
       "httptwitpiccom5j3uq    1\n",
       "dink                   1\n",
       "ca                     1\n",
       "normal                 1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(data['tweet']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jamielewislewis cant believe it really doesnt ...\n",
       "1    vodka tonic looking forward going saddle ranch...\n",
       "2    ddlovatofans1neg1 could follow pleasei would r...\n",
       "3    jordanknight once please tell us why u thinkin...\n",
       "4    had dream walk fast food resturaunt sold ice c...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Spelling correction   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jamielewislewis can believe it really doesn be...\n",
       "1    vodka tonic looking forward going saddle ranch...\n",
       "2    ddlovatofans1neg1 could follow please would re...\n",
       "3    jordanknight once please tell us why u thinkin...\n",
       "4    had dream walk fast food restraint sold ice cr...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob\n",
    "data['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tokenization for dividing the text into a sequence of words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WordList(['vodka', 'tonic', 'looking', 'forward', 'going', 'saddle', 'ranch', 'westgate', 'husband', 'sans', 'kiddos'])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "TextBlob(data['tweet'][1]).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stemming  for removal of suffices, like “ing”, “ly”, “s”, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jamielewislewi cant believ it realli doesnt be...\n",
       "1    vodka tonic look forward go saddl ranch westga...\n",
       "2    ddlovatofans1neg1 could follow pleasei would r...\n",
       "3    jordanknight onc pleas tell us whi u think person\n",
       "4    had dream walk fast food resturaunt sold ice c...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "data['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lemmatization to  converts the word into its root word, rather than just stripping the suffices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    jamielewislewis cant believe it really doesnt ...\n",
       "1    vodka tonic looking forward going saddle ranch...\n",
       "2    ddlovatofans1neg1 could follow pleasei would r...\n",
       "3    jordanknight once please tell u why u thinking...\n",
       "4    had dream walk fast food resturaunt sold ice c...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from textblob import Word\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2001x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7593 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word', stop_words= 'english',ngram_range=(1,1))\n",
    "tfidf.fit_transform(data['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bag of Words (BoW)  to  represent  text which describes the presence of words within the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(data['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.decomposition.truncated_svd import TruncatedSVD        \n",
    "pca = TruncatedSVD(n_components=2)                                \n",
    "X_reduced_train = pca.fit_transform(counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=counts\n",
    "Y=data.values[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import model_selection \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.6542857142857144\n",
      "CART 0.5792857142857142\n",
      "RF 0.6100000000000001\n",
      "NB 0.542142857142857\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name,model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    cv_result = model_selection.cross_val_score(model,X_train.toarray(),Y_train, cv = kfold, scoring = \"accuracy\")\n",
    "    names.append(name)\n",
    "    results.append(cv_result)\n",
    "for i in range(len(names)):\n",
    "    print(names[i],results[i].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since logistic regression has greater accuracy we are building model using logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR = LogisticRegression()\n",
    "LogR.fit(X_train,Y_train)\n",
    "Y_pred = LogR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200  99]\n",
      " [ 99 203]]\n",
      "0.670549084858569\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.67      0.67      0.67       299\n",
      "        pos       0.67      0.67      0.67       302\n",
      "\n",
      "avg / total       0.67      0.67      0.67       601\n",
      "\n",
      "Accuracy of the model: 0.670549084858569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "confusion_matrix=confusion_matrix(Y_test,Y_pred)\n",
    "print(confusion_matrix)\n",
    "print(accuracy_score(Y_test, Y_pred))\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "accuracy_score=accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy of the model:\",accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
